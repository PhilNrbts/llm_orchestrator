{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LLM Orchestrator CLI","text":"<p>The LLM Orchestrator is a powerful, local-first command-line tool designed for developers, researchers, and writers who need to interact with multiple Large Language Models (LLMs) efficiently. It provides a secure and flexible way to run parallel queries, create complex sequential chains, and manage common workflows directly from your terminal.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Secure API Key Management: Your API keys are encrypted locally in a <code>vault.enc</code> file. A master password decrypts them in memory only when needed.</li> <li>Parallel Queries: Send a single prompt to multiple models (like Gemini, Anthropic, etc.) at the same time and compare their responses side-by-side.</li> <li>On-the-Fly Chains: Construct powerful, sequential workflows directly on the command line. Use the output of one model as the input for the next, with custom roles and personas for each step.</li> <li>Saved Chains: Save your most-used workflows to a <code>config.yaml</code> file for easy reuse.</li> <li>Session Management: Start an authenticated session to avoid entering your password for every command.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Head over to the Installation guide to get started.</p>"},{"location":"about/core_concepts/","title":"Core Concepts","text":"<p>This project is built on a few core concepts that are important to understand.</p>"},{"location":"about/core_concepts/#security-the-encrypted-vault","title":"Security: The Encrypted Vault","text":"<p>Your API keys are sensitive credentials. To keep them safe, we use an encrypted vault (<code>vault.enc</code>).</p> <ul> <li>Encryption: The vault is encrypted using Fernet (AES-128-CBC).</li> <li>Master Password: A single master password is used to derive an encryption key. This password is the only way to unlock the vault.</li> <li>In-Memory Decryption: The vault is only ever decrypted in memory when a command is run. Your keys are never written to disk in plaintext.</li> </ul>"},{"location":"about/core_concepts/#configuration-files","title":"Configuration Files","text":"<p>The CLI is controlled by two main YAML files:</p> <ul> <li><code>models.yaml</code>: Defines the models available to the orchestrator, their settings (like <code>temperature</code>), and the available <code>roles</code> and <code>personas</code> for autocompletion.</li> <li><code>config.yaml</code>: Stores your saved chains. You can edit this file directly or manage it using the <code>my-cli chain save</code> command.</li> </ul>"},{"location":"about/core_concepts/#session-management","title":"Session Management","text":"<p>To avoid requiring your master password for every single command, the CLI uses a temporary, encrypted session file (<code>.session_cache</code>).</p> <ul> <li>When you run <code>my-cli auth start</code>, your password is encrypted and stored in this file.</li> <li>For the next 15 minutes, other commands can decrypt this file to get your password.</li> <li>After 15 minutes of inactivity, the session file is considered expired and will be deleted.</li> <li>Running <code>my-cli auth stop</code> will also delete the file immediately.</li> </ul>"},{"location":"about/legacy/","title":"Legacy Architecture: Docker &amp; Web API","text":"<p>This document archives the original architecture for this project, which was designed as a containerized web service. This approach has been deprecated in favor of a simpler, direct command-line interface to focus on core functionality.</p>"},{"location":"about/legacy/#original-concept","title":"Original Concept","text":"<p>The project was initially a secure, containerized, and cross-platform framework to interact with multiple Large Language Models (LLMs). It was designed to run within a Docker container, accessible via a FastAPI web server.</p>"},{"location":"about/legacy/#original-features","title":"Original Features","text":"<ul> <li>Secure API Key Storage: Used PBKDF2 for key derivation and Fernet (AES-128-CBC) for encrypting API keys.</li> <li>Dual Execution Modes: Flexible parallel or sequential processing of prompts.</li> <li>Containerized with Docker: Easy to set up and run consistently on any system with Docker support.</li> <li>Web API: Exposed a REST API using FastAPI for integration.</li> </ul>"},{"location":"about/legacy/#legacy-usage-web-api","title":"Legacy Usage (Web API)","text":"<p>Interaction was done via an HTTP client like <code>curl</code>.</p> <p>Parallel Query Example:</p> <pre><code>curl -X POST http://localhost:8000/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Explain quantum computing in simple terms\",\n    \"password\": \"your-secret-password\",\n    \"mode\": \"parallel\"\n  }'\n</code></pre>"},{"location":"usage/chains/","title":"Understanding Chains","text":"<p>Chains are the most powerful feature of the LLM Orchestrator. They allow you to create sophisticated workflows where the output of one model becomes the input for the next.</p>"},{"location":"usage/chains/#on-the-fly-chains","title":"On-the-Fly Chains","text":"<p>You can create a temporary chain directly from the command line using the <code>--step</code> flag with the <code>run</code> command. This is useful for experiments and one-off tasks.</p> <p>The syntax for a step is:</p> <pre><code>\"Role&gt;/Persona)-Model\"\n</code></pre> <ul> <li>Role: The action the model should take (e.g., <code>Critique</code>, <code>Summarize</code>, <code>GenerateCode</code>).</li> <li>Persona: The \"personality\" the model should adopt (e.g., <code>Developer</code>, <code>Poet</code>, <code>Tester</code>).</li> <li>Model: The LLM to use for this step (e.g., <code>gemini</code>, <code>anthropic</code>).</li> </ul>"},{"location":"usage/chains/#example","title":"Example","text":"<p>This example first generates code with Gemini and then asks Anthropic to critique it.</p> <pre><code>my-cli run -p \"Write a python function for fibonacci\" \\\n    --step \"GenerateCode&gt;/Developer)-gemini\" \\\n    --step \"Critique&gt;/Tester)-anthropic\"\n</code></pre>"},{"location":"usage/chains/#saved-chains","title":"Saved Chains","text":"<p>For workflows you use frequently, you can save them as a named chain in your <code>config.yaml</code> file.</p>"},{"location":"usage/chains/#saving-a-chain","title":"Saving a Chain","text":"<p>Use the <code>chain save</code> command:</p> <pre><code>my-cli chain save my_analysis_workflow \\\n    --step \"Summarize&gt;/TechnicalWriter)-gemini\" \\\n    --step \"Plan&gt;/ProductManager)-anthropic\"\n</code></pre> <p>This saves a chain named <code>my_analysis_workflow</code> to your <code>config.yaml</code>.</p>"},{"location":"usage/chains/#running-a-saved-chain","title":"Running a Saved Chain","text":"<p>Use the <code>chain run</code> command:</p> <pre><code>my-cli chain run my_analysis_workflow -p \"The latest report on AI trends\"\n</code></pre> <p>This will execute the two steps defined in the <code>my_analysis_workflow</code> chain.</p> <p>```</p>"},{"location":"usage/commands/","title":"Commands","text":"<p>The CLI is organized into several command groups. You can always get more information about a command by using the <code>--help</code> flag.</p>"},{"location":"usage/commands/#run","title":"<code>run</code>","text":"<p>The <code>run</code> command is the primary way to execute queries.</p>"},{"location":"usage/commands/#parallel-queries","title":"Parallel Queries","text":"<p>To send a prompt to multiple models at once, use the <code>--model</code> flag:</p> <pre><code>my-cli run -p \"What is the future of AI?\" --model gemini --model anthropic\n</code></pre>"},{"location":"usage/commands/#sequential-chains-on-the-fly","title":"Sequential Chains (On-the-Fly)","text":"<p>To create a chain of queries, use the <code>--step</code> flag. The format is <code>Role&gt;/Persona)-Model</code>.</p> <pre><code>my-cli run -p \"Write a python function for fibonacci\" \\\n    --step \"GenerateCode&gt;/Developer)-gemini\" \\\n    --step \"Critique&gt;/Tester)-anthropic\"\n</code></pre>"},{"location":"usage/commands/#chain","title":"<code>chain</code>","text":"<p>The <code>chain</code> command group lets you manage saved workflows.</p> <ul> <li><code>my-cli chain list</code>: Lists all chains saved in <code>config.yaml</code>.</li> <li><code>my-cli chain run &lt;chain_name&gt; -p \"Your prompt\"</code>: Executes a saved chain.</li> <li><code>my-cli chain save &lt;chain_name&gt; --step \"...\"</code>: Saves a new chain.</li> </ul>"},{"location":"usage/commands/#auth","title":"<code>auth</code>","text":"<p>The <code>auth</code> command group manages password sessions.</p> <ul> <li><code>my-cli auth start</code>: Starts a 15-minute session where you don't need to re-enter your password.</li> <li><code>my-cli auth stop</code>: Ends the current session.</li> </ul>"},{"location":"usage/commands/#install-completion","title":"<code>install-completion</code>","text":"<p>This command helps you set up shell autocompletion. See the Installation guide for details.</p> <p>```</p>"},{"location":"usage/installation/","title":"Installation","text":"<p>Getting the LLM Orchestrator CLI running on your system is straightforward.</p>"},{"location":"usage/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>First, clone the project repository from GitHub:</p> <pre><code>git clone https://github.com/your-username/llm_orchestrator.git\ncd llm_orchestrator\n</code></pre>"},{"location":"usage/installation/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>The project uses Python and a set of libraries defined in <code>requirements.txt</code>. It's highly recommended to use a virtual environment.</p> <pre><code># Create and activate a virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install the required packages\npip install -r requirements.txt\n</code></pre>"},{"location":"usage/installation/#3-set-up-the-api-key-vault","title":"3. Set Up the API Key Vault","text":"<p>Your LLM API keys are stored securely in an encrypted vault. To initialize the vault and add your keys, run the interactive setup script:</p> <pre><code>python -m scripts.init_vault\n</code></pre> <p>The script will prompt you for your master password and then for each API key you wish to add.</p>"},{"location":"usage/installation/#4-optional-install-shell-completion","title":"4. (Optional) Install Shell Completion","text":"<p>For a much better user experience, install the shell completion script. This will enable tab-completion for commands, options, and even chain names.</p> <p>Run the following command for your shell:</p> <p>=== \"Bash\"     <code>bash     my-cli install-completion --shell bash</code>     Then add this to your <code>~/.bashrc</code>:     <code>bash     eval \"$(_MY-CLI_COMPLETE=bash_source my-cli)\"</code></p> <p>=== \"Zsh\"     <code>bash     my-cli install-completion --shell zsh</code>     Then add this to your <code>~/.zshrc</code>:     <code>bash     eval \"$(_MY-CLI_COMPLETE=zsh_source my-cli)\"</code></p> <p>=== \"Fish\"     <code>bash     my-cli install-completion --shell fish</code>     Then add this to your <code>~/.config/fish/config.fish</code>:     <code>bash     eval (env _MY-CLI_COMPLETE=fish_source my-cli)</code></p> <p>After updating your shell's configuration file, restart your shell for the changes to take effect.</p>"}]}
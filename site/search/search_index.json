{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LLM Orchestrator CLI","text":"<p>The LLM Orchestrator is a powerful, local-first command-line tool designed for developers, researchers, and writers who need to interact with multiple Large Language Models (LLMs) efficiently. It provides a secure and flexible way to run queries and manage workflows directly from your terminal.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Interactive Chat: The primary way to use the application. A rich terminal UI with a command dropdown, chat history, and model details.</li> <li>Secure API Key Management: Your API keys are encrypted locally in a <code>vault.enc</code> file. A master password decrypts them in memory only when needed.</li> <li>Parallel Queries: Send a single prompt to multiple models (like Gemini, Anthropic, etc.) at the same time and compare their responses side-by-side.</li> <li>On-the-Fly Chains: Construct powerful, sequential workflows directly on the command line. Use the output of one model as the input for the next, with custom roles and personas for each step.</li> <li>Saved Chains: Save your most-used workflows to a <code>config.yaml</code> file for easy reuse.</li> <li>Session Management: Start an authenticated session to avoid entering your password for every command.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Head over to the Installation guide to get started.</p>"},{"location":"about/core_concepts/","title":"Core Concepts","text":"<p>This project is built on a few core concepts that are important to understand.</p>"},{"location":"about/core_concepts/#security-the-encrypted-vault","title":"Security: The Encrypted Vault","text":"<p>Your API keys are sensitive credentials. To keep them safe, we use an encrypted vault (<code>vault.enc</code>).</p> <ul> <li>Encryption: The vault is encrypted using Fernet (AES-128-CBC).</li> <li>Master Password: A single master password is used to derive an encryption key. This password is the only way to unlock the vault.</li> <li>In-Memory Decryption: The vault is only ever decrypted in memory when a command is run. Your keys are never written to disk in plaintext.</li> </ul>"},{"location":"about/core_concepts/#configuration-files","title":"Configuration Files","text":"<p>The CLI is controlled by two main YAML files:</p> <ul> <li><code>models.yaml</code>: Defines the models available to the orchestrator, their settings (like <code>temperature</code>), and the available <code>roles</code> and <code>personas</code> for autocompletion.</li> <li><code>config.yaml</code>: Stores your saved chains. You can edit this file directly or manage it using the <code>my-cli chain save</code> command.</li> </ul>"},{"location":"about/core_concepts/#session-management","title":"Session Management","text":"<p>To avoid requiring your master password for every single command, the CLI uses a temporary, encrypted session file (<code>.session_cache</code>).</p> <ul> <li>When you run <code>my-cli auth start</code>, your password is encrypted and stored in this file.</li> <li>For the next 15 minutes, other commands can decrypt this file to get your password.</li> <li>After 15 minutes of inactivity, the session file is considered expired and will be deleted.</li> <li>Running <code>my-cli auth stop</code> will also delete the file immediately.</li> </ul>"},{"location":"about/legacy/","title":"Legacy Architecture: Docker &amp; Web API","text":"<p>This document archives the original architecture for this project, which was designed as a containerized web service. This approach has been deprecated in favor of a simpler, direct command-line interface to focus on core functionality.</p>"},{"location":"about/legacy/#original-concept","title":"Original Concept","text":"<p>The project was initially a secure, containerized, and cross-platform framework to interact with multiple Large Language Models (LLMs). It was designed to run within a Docker container, accessible via a FastAPI web server.</p>"},{"location":"about/legacy/#original-features","title":"Original Features","text":"<ul> <li>Secure API Key Storage: Used PBKDF2 for key derivation and Fernet (AES-128-CBC) for encrypting API keys.</li> <li>Dual Execution Modes: Flexible parallel or sequential processing of prompts.</li> <li>Containerized with Docker: Easy to set up and run consistently on any system with Docker support.</li> <li>Web API: Exposed a REST API using FastAPI for integration.</li> </ul>"},{"location":"about/legacy/#legacy-usage-web-api","title":"Legacy Usage (Web API)","text":"<p>Interaction was done via an HTTP client like <code>curl</code>.</p> <p>Parallel Query Example:</p> <pre><code>curl -X POST http://localhost:8000/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Explain quantum computing in simple terms\",\n    \"password\": \"your-secret-password\",\n    \"mode\": \"parallel\"\n  }'\n</code></pre>"},{"location":"usage/chains/","title":"Understanding Chains","text":"<p>Chains are the most powerful feature of the LLM Orchestrator. They allow you to create sophisticated workflows where the output of one model becomes the input for the next.</p>"},{"location":"usage/chains/#on-the-fly-chains","title":"On-the-Fly Chains","text":"<p>You can create a temporary chain directly from the command line using the <code>--step</code> flag with the <code>run</code> command. This is useful for experiments and one-off tasks.</p> <p>The syntax for a step is:</p> <pre><code>\"Role&gt;/Persona)-Model\"\n</code></pre> <ul> <li>Role: The action the model should take (e.g., <code>Critique</code>, <code>Summarize</code>, <code>GenerateCode</code>).</li> <li>Persona: The \"personality\" the model should adopt (e.g., <code>Developer</code>, <code>Poet</code>, <code>Tester</code>).</li> <li>Model: The LLM to use for this step (e.g., <code>gemini</code>, <code>anthropic</code>).</li> </ul>"},{"location":"usage/chains/#example","title":"Example","text":"<p>This example first generates code with Gemini and then asks Anthropic to critique it.</p> <pre><code>my-cli run -p \"Write a python function for fibonacci\" \\\n    --step \"GenerateCode&gt;/Developer)-gemini\" \\\n    --step \"Critique&gt;/Tester)-anthropic\"\n</code></pre>"},{"location":"usage/chains/#saved-chains","title":"Saved Chains","text":"<p>For workflows you use frequently, you can save them as a named chain in your <code>config.yaml</code> file.</p>"},{"location":"usage/chains/#saving-a-chain","title":"Saving a Chain","text":"<p>Use the <code>chain save</code> command:</p> <pre><code>my-cli chain save my_analysis_workflow \\\n    --step \"Summarize&gt;/TechnicalWriter)-gemini\" \\\n    --step \"Plan&gt;/ProductManager)-anthropic\"\n</code></pre> <p>This saves a chain named <code>my_analysis_workflow</code> to your <code>config.yaml</code>.</p>"},{"location":"usage/chains/#running-a-saved-chain","title":"Running a Saved Chain","text":"<p>Use the <code>chain run</code> command:</p> <pre><code>my-cli chain run my_analysis_workflow -p \"The latest report on AI trends\"\n</code></pre> <p>This will execute the two steps defined in the <code>my_analysis_workflow</code> chain.</p> <p>```</p>"},{"location":"usage/commands/","title":"Commands","text":"<p>The CLI is organized into several command groups. You can always get more information about a command by using the <code>--help</code> flag.</p>"},{"location":"usage/commands/#interactive-chat","title":"Interactive Chat","text":"<p>The primary way to use the application is through the interactive chat. To start the chat, simply run the application without any commands:</p> <pre><code>llm-orchestrator\n</code></pre>"},{"location":"usage/commands/#slash-commands","title":"Slash Commands","text":"<p>The interactive chat has a number of slash commands to control the application:</p> <ul> <li><code>/help</code>: Show a list of available commands.</li> <li><code>/changemodel</code>: Change the model used in the chat.</li> <li><code>/changeprovider</code>: Change the provider used in the chat.</li> <li><code>/list</code>: List the available models for the current provider.</li> <li><code>/config</code>: Show the current configuration.</li> <li><code>/mode</code>: Change the chat mode.</li> <li><code>/exit</code> or <code>/quit</code>: Exit the chat.</li> </ul>"},{"location":"usage/commands/#run","title":"<code>run</code>","text":"<p>The <code>run</code> command is a secondary way to execute queries.</p>"},{"location":"usage/commands/#parallel-queries","title":"Parallel Queries","text":"<p>To send a prompt to multiple models at once, use the <code>--model</code> flag:</p> <pre><code>llm-orchestrator run -p \"What is the future of AI?\" --model gemini --model anthropic\n</code></pre>"},{"location":"usage/commands/#sequential-chains-on-the-fly","title":"Sequential Chains (On-the-Fly)","text":"<p>To create a chain of queries, use the <code>--step</code> flag. The format is <code>Role&gt;/Persona)-Model</code>.</p> <pre><code>llm-orchestrator run -p \"Write a python function for fibonacci\" \\\n    --step \"GenerateCode&gt;/Developer)-gemini\" \\\n    --step \"Critique&gt;/Tester)-anthropic\"\n</code></pre>"},{"location":"usage/commands/#chain","title":"<code>chain</code>","text":"<p>The <code>chain</code> command group lets you manage saved workflows.</p> <ul> <li><code>llm-orchestrator chain list</code>: Lists all chains saved in <code>config.yaml</code>.</li> <li><code>llm-orchestrator chain run &lt;chain_name&gt; -p \"Your prompt\"</code>: Executes a saved chain.</li> <li><code>llm-orchestrator chain save &lt;chain_name&gt; --step \"...\"</code>: Saves a new chain.</li> </ul>"},{"location":"usage/commands/#auth","title":"<code>auth</code>","text":"<p>The <code>auth</code> command group manages password sessions.</p> <ul> <li><code>llm-orchestrator auth start</code>: Starts a 15-minute session where you don't need to re-enter your password.</li> <li><code>llm-orchestrator auth stop</code>: Ends the current session.</li> </ul>"},{"location":"usage/configuration/","title":"Configuration","text":"<p>The LLM Orchestrator CLI is configured through two YAML files: <code>config.yaml</code> and <code>models.yaml</code>.</p>"},{"location":"usage/configuration/#configyaml","title":"<code>config.yaml</code>","text":"<p>This file stores the main configuration for the application.</p> <ul> <li><code>main_llm</code>: This section defines the default provider and model to be used when the application starts.</li> <li><code>chains</code>: This section stores any saved chains that you have created.</li> </ul> <p>Here is an example <code>config.yaml</code> file:</p> <pre><code>main_llm:\n  provider: gemini\n  model: gemini-1.5-flash-latest\nchains:\n  my_chain:\n    - \"GenerateCode&gt;/Developer)-gemini\"\n    - \"Critique&gt;/Tester)-anthropic\"\n</code></pre>"},{"location":"usage/configuration/#modelsyaml","title":"<code>models.yaml</code>","text":"<p>This file stores the configuration for the different providers and models that the application can use.</p> <ul> <li>Provider: Each top-level key in this file is a provider (e.g., <code>gemini</code>, <code>anthropic</code>).</li> <li><code>api_key_name</code>: This is the name of the API key that the application will look for in the vault.</li> <li><code>default_model</code>: This is the default model that will be used for this provider if no other model is specified.</li> <li><code>models</code>: This is a list of the models that are available for this provider. Each model can have its own specific parameters (e.g., <code>max_tokens</code>, <code>temperature</code>).</li> </ul> <p>Here is an example <code>models.yaml</code> file:</p> <pre><code>anthropic:\n  api_key_name: ANTHROPIC_API_KEY\n  default_model: claude-3-5-sonnet-20240620\n  models:\n    - name: claude-3-5-sonnet-20240620\n      max_tokens: 4096\n    - name: claude-3-haiku-20240307\n      max_tokens: 1000\n\ngemini:\n  api_key_name: GEMINI_API_KEY\n  default_model: gemini-1.5-flash-latest\n  models:\n    - name: gemini-1.5-flash-latest\n      temperature: 0.7\n    - name: gemini-1.5-pro-latest\n      temperature: 0.7\n</code></pre>"},{"location":"usage/getting_started/","title":"Getting Started","text":"<p>This guide will walk you through the process of installing and configuring the LLM Orchestrator CLI.</p>"},{"location":"usage/getting_started/#1-installation","title":"1. Installation","text":"<p>First, you'll need to have Poetry installed. Once you have Poetry, you can install the application with the following command:</p> <pre><code>poetry install\n</code></pre>"},{"location":"usage/getting_started/#2-initializing-the-vault","title":"2. Initializing the Vault","text":"<p>The first time you run the application, you'll need to initialize the vault. The vault is an encrypted file that stores your API keys. To initialize the vault, run the following command:</p> <pre><code>poetry run python -m scripts.init_vault\n</code></pre> <p>You will be prompted to create a master password. This password will be used to encrypt and decrypt your API keys.</p>"},{"location":"usage/getting_started/#3-adding-api-keys","title":"3. Adding API Keys","text":"<p>After you've initialized the vault, you'll need to add your API keys. You can do this by running the <code>vault_manager.py</code> script:</p> <pre><code>poetry run python -m scripts.vault_manager\n</code></pre> <p>You will be prompted for your master password, and then you will be able to add, update, or delete API keys.</p>"},{"location":"usage/getting_started/#4-running-the-application","title":"4. Running the Application","text":"<p>Once you've added your API keys, you can run the application with the following command:</p> <pre><code>poetry run python -m app.main\n</code></pre> <p>You will be prompted for your master password, and then you will be dropped into the interactive chat.</p>"},{"location":"usage/getting_started/#bypassing-the-password-prompt","title":"Bypassing the Password Prompt","text":"<p>If you are in an environment where the password prompt is not supported, you can use the <code>--no-mask</code> flag:</p> <pre><code>poetry run python -m app.main --no-mask\n</code></pre>"},{"location":"usage/installation/","title":"Installation","text":"<p>This project is managed with Poetry, which handles dependency management and virtual environments automatically.</p>"},{"location":"usage/installation/#1-install-poetry","title":"1. Install Poetry","text":"<p>First, if you don't have Poetry, install it by following the official instructions.</p>"},{"location":"usage/installation/#2-clone-the-repository","title":"2. Clone the Repository","text":"<p>Clone the project repository from GitHub:</p> <pre><code>git clone https://github.com/your-username/llm_orchestrator.git\ncd llm_orchestrator\n</code></pre>"},{"location":"usage/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Poetry will read the <code>pyproject.toml</code> file, create a virtual environment, and install all necessary dependencies with a single command:</p> <pre><code>poetry install\n</code></pre>"},{"location":"usage/installation/#4-set-up-the-api-key-vault","title":"4. Set Up the API Key Vault","text":"<p>Your LLM API keys are stored securely in an encrypted vault. To initialize the vault and add your keys, run the settings manager:</p> <pre><code>poetry run python settings.py\n</code></pre> <p>Select [1] Initialize a new vault and follow the prompts.</p>"},{"location":"usage/installation/#5-optional-install-shell-completion","title":"5. (Optional) Install Shell Completion","text":"<p>For a much better user experience, install the shell completion script. This will enable tab-completion for commands, options, and even chain names.</p> <p>Run the following command for your shell (using <code>poetry run</code>):</p> <p>=== \"Bash\"     <code>bash     poetry run python -m app.main install-completion --shell bash</code>     Then add this to your <code>~/.bashrc</code>:     <code>bash     eval \"$(_LLM_ORCHESTRATOR_CLI_COMPLETE=bash_source llm-orchestrator-cli)\"</code></p> <p>=== \"Zsh\"     <code>bash     poetry run python -m app.main install-completion --shell zsh</code>     Then add this to your <code>~/.zshrc</code>:     <code>bash     eval \"$(_LLM_ORCHESTRATOR_CLI_COMPLETE=zsh_source llm-orchestrator-cli)\"</code></p> <p>=== \"Fish\"     <code>bash     poetry run python -m app.main install-completion --shell fish</code>     Then add this to your <code>~/.config/fish/config.fish</code>:     <code>bash     eval (env _LLM_ORCHESTRATOR_CLI_COMPLETE=fish_source llm-orchestrator-cli)</code></p> <p>After updating your shell's configuration file, restart your shell for the changes to take effect.</p>"}]}